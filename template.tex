\documentclass{report}

\input{preamble}
\input{macros}
\input{letterfonts}

\title{\Huge{36225}\\Probability}
\author{\huge{ZhenTing Liu}}
\date{}

\begin{document}

\maketitle
\newpage% or \cleardoublepage
% \pdfbookmark[<level>]{<title>}{<dest>}
\pdfbookmark[section]{\contentsname}{toc}
\tableofcontents
\pagebreak
\chapter*{Laws of probability}
\chapter{}
\section{independence}
\nt{Two events A,B are said to be independent if P(A$|$B) = P(A) \textbf{and} P(B$|$A) = P(B), note that this is a conjunction not a disjunction, if only one of the two conditions is satisfied, it is not sufficient enough to conclude A,B's independence.
Naturally, to tell if two evens are dependent,we simply negate above statement, yielding A,B dependent if P(A$|$B) $\neq$ P(A) \textbf{or} P(B$|$A) $\neq$ P(B)}

\dfn{}{
	consider two events A,B then
	P(B) = P(B$|$A)P(A) + P(B$|$A$^{c}$)P(A$^{c}$), apply bayes rule we have $$P(A|B) = \frac{P(B|A)P(A)}{P(B|A)P(A)+P(B|A^{c})P(A^{c})}$$
	more generally
	$$P(A_{i}|B) = \frac{P(B|A_{i})P(A_{i})}{P(B|A_{1})P(A_{1}) + \dots + P(B|A_{n})P(A_{n})}$$
}

\chapter*{Random Variables}
\chapter{}
\section{Probability and Mass functions}

A \textbf{discrete random variable} X on the probability space $(\Omega,\mathcal{F}, \mathbb{P})$ is defined to be a mapping $X: \Omega \mapsto \mathbb{R}$ such that
\begin{center}
	the image $X(\Omega)$ is a countable subset of $\mathbb{R}$ and \\ $\{\omega \in \Omega: X(\omega) = x\} \in \mathcal{F}$
\end{center}
A discrete random variable X takes value in $\mathbb{R}$, but we cannot predict the actual value of X with certainty since the underlying experiment involves chance. Instead, we would like to measure
the probability that X takes a given value, x say. 

The most interesting things about a discrete random variable are the values that it may take and the probability associated with these values. If X is a discrete random variable on the probability space $(\Omega, \mathcal{F}, \mathbb{P})$, then its \textbf{image} $\img$X is the image of $\Omega$ under X, that is, the set of values taken by X.

Henceforth, we abbreviate events of the form $\{\omega \in \Omega: X(\omega) = x\}$ to the more convenient form $\{X = x\}$.

\dfn{The (probability) mass function}
{
	the pmf of the discrete random variable X is the function $p_{X}: \mathbb{R} \mapsto [0,1]$ defined by
	\begin{center}
		$p_{X}(x) = \mathbb{P}(X = x)$
	\end{center}
}
Thus, $p_{X}(x)$ is the probability that the mapping X takes the value $x$. Note that $\img$X is countable for any discrete random variable X, and 
\begin{align}
	p_{X}(x) &= 0 \quad \text{if $x \notin \img$ X,}\\
	\sum_{x\in \img X}^{}p_{X}(x) &= \mathbb{P}(\bigcup_{x \in \img X} \{\omega \in \Omega: X(\omega) = x\}) = \mathbb{P}(\mathbb{\Omega}) = 1 \\
	0 &\leq p_{X}(x) \leq 1 \forall x
\end{align}

To simplify things, here are my understanding of random variable with less rigrous math definitions. 

A random variable is an abstract way to talk about experimental outcomes, which makes
it possible to flexibly apply probability theory. Note that you cannot observe a random
variable X itself, i.e., you cannot observe the function that maps experimental outcomes to
numbers. The experimenter defines the random variable (i.e., function) of interest, and then
observes the result of applying the defined function to an experimental outcome

\begin{note}
	The realization of a random variable is the result of applying the random variable (i.e., function) to an observed outcome of a random experiment. This is what the
experimenter actually observes. The realization of a random variable is typically denoted
using lowercase italicized Roman letters, e.g., $x$ is a realization of X.

\end{note}

\ex{}{Suppose we flip a fair (two-sided) coin $n \geq 2$ times, and assume that the $n$
flips are independent of one another. Define X as the number of coin flips that are heads.
Note that X is a random variable given that it is a function (i.e., counting the number of
heads) that is applied to a random process (i.e., independently flipping a fair coin $n$ times).
Possible realizations of the random variable X include any $x \in \{0, 1, . . . , n\}$, i.e., we could
observe any number of heads between $0$ and $n$.}

\section{Probability Mass and Density Functions}

\dfn{Probability mass function}{The probability mass function (PMF) of a discrete random variable X is the
function $f(\cdot)$ that associates a probability with each $x \in S$. In other words, the PMF of X
is the function that returns $P(X = x)$ for each x in the domain of X.}

Any PMF must define a valid probability distribution, with the properties:
\begin{align}
	f(x) &= P(X = x) \geq 0 \quad \forall x \in S \\
	\sum_{x\in S}^{}f(x) &= 1
\end{align}

\ex{}{See below for the PMF corresponding to the coin flipping example with
$n = 5$ and $n = 10$ independent flips. As we might expect, the most likely realizations of
X are those in which we observe approximately $n/2$ heads. Note that it is still possible to
observe $x = 0$ or $x = n$ heads, but these extreme results are less likely to occur.

\includegraphics*[scale = 0.5]{Screenshot 2024-09-13 181832.png}
}


\dfn{Probability density function}{The probability density function (PDF) of a continuous random variable X is
the function $f(\cdot)$ that associates a probability with each range of realizations of X. The area
under the PDF between $a$ and $b$ returns $P(a < X < b)$ for any $a, b \in S$ satisfying $a < b$.}

Any PDF must have properties:
\begin{align*}
	f(x) &\geq 0 \quad \forall x \in S \\
	\int_{a}^{b}f(x)dx &= P(a<X<b) \geq 0 \quad \forall a,b \in S \quad \text{satisfying} \quad a<b \\
	\int_{x \in S}^{}f(x)dx &= 1
\end{align*}

Note that PMF is inclusively bounded above by $1$ while PDF does not specify an upper bound.

\ex{}{Suppose that we randomly spin the second hand around a clock face $n$ independent times. Define $Z_{i}$ as the position where the second hand stops spinning on the i-th
replication, and define $X = \frac{1}{n}\sum_{i=1}^{n}Z_{i}$ as the average of the $n$ spin results. Note that the
realizations of X are any values $x \in [0, 12]$, which is the same domain as $Z_{i}$
for $i = 1,\dots, n$. See figure below for the PDF with $n = 1$ and $n = 5$ independent spins. Note that with $n = 1$
spin, the PDF is simply a flat line between $0$ and $12$, which implies that $P(x < X < x + 1)$
is equal for any $x \in \{0,\dots,11\}$. This makes sense given that, for any given spin, the second
hand could land anywhere on the clock face with equal probability. With $n = 5$ spins, the
PDF has a bell shape, where values around the midpoint of $x = 6$ have the largest density. With $n = 5$ spins, we have that $\int_{4}^{8}f(x)dx = P(4 < X < 8) = 0.80$ (i.e., about $80 \%$ of the
realizations of X are between $4$ and $8$).

\includegraphics*[scale = 0.6]{fig3.png}
}

\section{Cumulative Distribution Function}
\dfn{CDF}{The cumulative distribution function (CDF) of a random variable X is the
function $F(\cdot)$ that returns the probability $P(X \leq x)$ for any $x \in S$. Note that CDF is a function from S to $[0, 1]$, i.e., $F : S \mapsto [0, 1]$.}

Any CDF must define a valid probability distribution, i.e., $0 \leq F(x) \leq 1$ for any $x \in S$,
which comes from the fact that $F(x) = P(X \leq x)$ is a probability calculation. Note that a
CDF can be defined for both discrete and continuous random variables:
\begin{align*}
	F(x) &= \sum_{z\in S, z \leq x}^{}f(x) \quad \text{for discrete random variables}\\
	F(x) &= \int_{-\infty}^{x}f(z)dz \quad \text{for continuous random variables}
\end{align*}
Furthermore, note that probabilities can be written in terms of the CDF, such as
\begin{center}
	$P(a < X \leq b) = F(b) - F(a)$
\end{center}

given that the CDF is related to the PMF(or PDF), such as 
\begin{align*}
	f(x) &= F(x) - \lim_{a \rightarrow x^{-}} F(a) \quad \text{for discrete case} \\
	f(x) &= \dfrac{dF(x)}{dx} \quad \text{for continuous random var}
\end{align*}

\section{Expectation}
Consider a fair die. If it were thrown a large amount of times, each of the possible outcomes $1,2,\dots,6$ would appear
on about one-sixth of the throws, and the average of the numbers observed would be approximately 
\begin{center}
	$\frac{1}{6} + \frac{1}{6}(2) + \frac{1}{6}(3) + \dots + \frac{1}{6}(6) = \frac{7}{2}$
\end{center}
which we call \textbf{mean value}. This notion of mean value is easily extended to more general distributions as follows.

\dfn{}{If X is a \textbf{discrete random variable}, the expectation of X is denoted by $\mathbb{E}(X)$ and defined by
\begin{center}
	$\mathbb{E}(X) = \sum_{x \in \img X}^{}x \mathbb{P}(X=x)$
\end{center}
}
and is often rewritten as 
\begin{center}
	$\mathbb{E}(X) = \sum_{x}^{}x\mathbb{P}(X=x) = \sum_{x}^{}xp_{X}(x)$
\end{center}

\chapter{Exam prep}
\section{Conceptual notes}
\nt{$P(A|B)P(B) = P(B|A)P(A)$}
% \section{Random}
% \dfn{Normed Linear Space and Norm $\boldsymbol{\|\cdot\|}$}{Let $V$ be a vector space over $\bbR$ (or $\bbC$). A norm on $V$ is function $\|\cdot\|\ V\to \bbR_{\geq 0}$ satisfying \begin{enumerate}[label=\bfseries\tiny\protect\circled{\small\arabic*}]
% 		\item \label{n:1}$\|x\|=0 \iff x=0$ $\forall$ $x\in V$
% 		\item \label{n:2}	$\|\lambda x\|=|\lambda|\|x\|$ $\forall$ $\lambda\in\bbR$(or $\bbC$), $x\in V$
% 		\item \label{n:3} $\|x+y\| \leq \|x\|+\|y\|$ $\forall$ $x,y\in V$ (Triangle Inequality/Subadditivity)
% 	\end{enumerate}And $V$ is called a normed linear space.

% 	$\bullet $ Same definition works with $V$ a vector space over $\bbC$ (again $\|\cdot\|\to\bbR_{\geq 0}$) where \ref{n:2} becomes $\|\lambda x\|=|\lambda|\|x\|$ $\forall$ $\lambda\in\bbC$, $x\in V$, where for $\lambda=a+ib$, $|\lambda|=\sqrt{a^2+b^2}$ }


% \ex{$\bs{p-}$Norm}{\label{pnorm}$V={\bbR}^m$, $p\in\bbR_{\geq 0}$. Define for $x=(x_1,x_2,\cdots,x_m)\in\bbR^m$ $$\|x\|_p=\Big(|x_1|^p+|x_2|^p+\cdots+|x_m|^p\Big)^{\frac1p}$$(In school $p=2$)}
% \textbf{Special Case $\bs{p=1}$}: $\|x\|_1=|x_1|+|x_2|+\cdots+|x_m|$ is clearly a norm by usual triangle inequality. \par
% \textbf{Special Case $\bs{p\to\infty\ (\bbR^m$ with $\|\cdot\|_{\infty})}$}: $\|x\|_{\infty}=\max\{|x_1|,|x_2|,\cdots,|x_m|\}$\\
% For $m=1$ these $p-$norms are nothing but $|x|$.
% Now exercise
% \qs{}{\label{exs1}Prove that triangle inequality is true if $p\geq 1$ for $p-$norms. (What goes wrong for $p<1$ ?)}
% \sol{\textbf{For Property \ref{n:3} for norm-2}	\subsubsection*{\textbf{When field is $\bbR:$}} We have to show\begin{align*}
% 		         & \sum_i(x_i+y_i)^2\leq \left(\sqrt{\sum_ix_i^2} +\sqrt{\sum_iy_i^2}\right)^2                                       \\
% 		\implies & \sum_i (x_i^2+2x_iy_i+y_i^2)\leq \sum_ix_i^2+2\sqrt{\left[\sum_ix_i^2\right]\left[\sum_iy_i^2\right]}+\sum_iy_i^2 \\
% 		\implies & \left[\sum_ix_iy_i\right]^2\leq \left[\sum_ix_i^2\right]\left[\sum_iy_i^2\right]
% 	\end{align*}So in other words prove $\langle x,y\rangle^2 \leq \langle x,x\rangle\langle y,y\rangle$ where
% 	$$\langle x,y\rangle =\sum\limits_i x_iy_i$$

% 	\begin{note}
% 		\begin{itemize}
% 			\item $\|x\|^2=\langle x,x\rangle$
% 			\item $\langle x,y\rangle=\langle y,x\rangle$
% 			\item $\langle \cdot,\cdot\rangle$ is $\bbR-$linear in each slot i.e. \begin{align*}
% 				      \langle rx+x',y\rangle=r\langle x,y\rangle+\langle x',y\rangle	\text{ and similarly for second slot}
% 			      \end{align*}Here in $\langle x,y\rangle$ $x$ is in first slot and $y$ is in second slot.
% 		\end{itemize}
% 	\end{note}Now the statement is just the Cauchy-Schwartz Inequality. For proof $$\langle x,y\rangle^2\leq \langle x,x\rangle\langle y,y\rangle $$ expand everything of $\langle x-\lambda y,x-\lambda y\rangle$ which is going to give a quadratic equation in variable $\lambda $ \begin{align*}
% 		\langle x-\lambda y,x-\lambda y\rangle & =\langle x,x-\lambda y\rangle-\lambda\langle y,x-\lambda y\rangle                                       \\
% 		                                       & =\langle x ,x\rangle -\lambda\langle x,y\rangle -\lambda\langle y,x\rangle +\lambda^2\langle y,y\rangle \\
% 		                                       & =\langle x,x\rangle -2\lambda\langle x,y\rangle+\lambda^2\langle y,y\rangle
% 	\end{align*}Now unless $x=\lambda y$ we have $\langle x-\lambda y,x-\lambda y\rangle>0$ Hence the quadratic equation has no root therefore the discriminant is greater than zero.

% 	\subsubsection*{\textbf{When field is $\bbC:$}}Modify the definition by $$\langle x,y\rangle=\sum_i\overline{x_i}y_i$$Then we still have $\langle x,x\rangle\geq 0$}

% \section{Algorithms}
% \begin{algorithm}[H]
% \KwIn{This is some input}
% \KwOut{This is some output}
% \SetAlgoLined
% \SetNoFillComment
% \tcc{This is a comment}
% \vspace{3mm}
% some code here\;
% $x \leftarrow 0$\;
% $y \leftarrow 0$\;
% \uIf{$ x > 5$} {
%     x is greater than 5 \tcp*{This is also a comment}
% }
% \Else {
%     x is less than or equal to 5\;
% }
% \ForEach{y in 0..5} {
%     $y \leftarrow y + 1$\;
% }
% \For{$y$ in $0..5$} {
%     $y \leftarrow y - 1$\;
% }
% \While{$x > 5$} {
%     $x \leftarrow x - 1$\;
% }
% \Return Return something here\;
% \caption{what}
% \end{algorithm}

\end{document}
